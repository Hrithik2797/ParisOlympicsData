# ParisOlympicsData

# Azure End-to-End Data Engineering Project
This project provides a comprehensive, beginner-friendly walkthrough of building an end-to-end data engineering solution on the Azure cloud platform. It integrates Azure Data Factory, Azure Databricks, Azure Data Lake, PySpark, and Azure DevOps CI/CD practices to demonstrate a real-world ETL pipeline from ingestion to transformation and orchestration.

Whether you're preparing for a data engineering role or looking to understand how modern data pipelines are implemented in the cloud, this project covers practical use cases and industry-relevant tools.

# ðŸ”§ Tech Stack
Azure Data Factory (ADF) â€“ Ingestion and orchestration

Azure Databricks â€“ Big data transformation using PySpark

Azure Data Lake Storage Gen2 â€“ Scalable cloud storage

Azure DevOps â€“ CI/CD integration with Git

PySpark â€“ Scalable distributed data processing

ARM Templates â€“ Infrastructure as code

# ðŸŽ¯ Project Objectives
Understand the architecture of an enterprise-grade data pipeline on Azure

Set up an Azure environment using a free account

Ingest data from GitHub using ADF

Build parametrized and reusable ETL pipelines

Perform real-time and batch transformations using PySpark

Implement CI/CD workflows using Azure DevOps

Create notebooks, workflows, and streaming jobs in Databricks

Apply Change Data Capture (CDC) with Spark Structured Streaming

Deploy Delta Live Tables for robust data pipelines
